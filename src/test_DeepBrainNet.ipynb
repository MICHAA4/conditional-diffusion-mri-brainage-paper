{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5ed071-6234-449a-87b7-0ba98d5e1181",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce97fd-f092-4e6e-860f-af35d962b96b",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d48a51-3f0a-4674-9be8-e0a46cc6c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948acaeb-41b8-4b41-8a5c-f50d053aa29e",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1f4ce-b8ad-46d6-9194-cf1060783a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import PIL\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import monai\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset, CacheDataset, DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Lambdad,\n",
    "    Resized,\n",
    "    Randomizable,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityRanged,\n",
    "    RepeatChanneld,\n",
    "    Transposed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf217a-f290-45ce-bb98-e270b37f5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)\n",
    "\n",
    "set_determinism(42)\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30960f57-6b1e-436c-bb07-ceaf18d34343",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"We got a GPU\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"Sorry, no GPU for you...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b26d6-f005-4332-874b-37a404ed882c",
   "metadata": {},
   "source": [
    "### Pytorch to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdff4b-1ae9-4aab-aa29-957b667bd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_to_numpy(data_loader):\n",
    "    images, ages = [], []\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        batch_cp = copy.deepcopy(batch)  \n",
    "        \n",
    "        img_batch = batch_cp[\"image\"].numpy()  # (batch, 96, 128, 3)\n",
    "        age_batch = batch_cp[\"age\"].numpy()  # (batch, 1)\n",
    "        \n",
    "        del batch\n",
    "        images.append(img_batch)\n",
    "        ages.append(age_batch)\n",
    "        \n",
    "    return np.concatenate(images), np.concatenate(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9ff5f-15f6-45af-9c9f-63a0a2e71ff0",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570d28e-0fe9-4015-8565-36fffe33485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "# finetuned\n",
    "checkpoint_path = './DBN_finetuned/best_90.h5'\n",
    "\n",
    "# original\n",
    "#checkpoint_path = './DeepBrainNet/Models/DBN_model.h5'\n",
    "model = load_model(checkpoint_path, compile=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9a0c8-4c07-4d66-b486-285d18276077",
   "metadata": {},
   "source": [
    "### Dataset & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6df9b9-44bc-40dc-ad04-a2c3dafaccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Randomizable, CacheDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        csv_file,\n",
    "        section,\n",
    "        transform=None,\n",
    "        seed=0,\n",
    "        cache_num=sys.maxsize,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=0,\n",
    "        progress: bool = True,\n",
    "    ) -> None:\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(\"Root directory root_dir must be a directory.\")\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.section = section\n",
    "        self.set_random_state(seed=seed)\n",
    "\n",
    "        data = self._generate_data_list()\n",
    "\n",
    "        CacheDataset.__init__(\n",
    "            self,\n",
    "            data=data,\n",
    "            transform=transform,\n",
    "            cache_num=cache_num,\n",
    "            cache_rate=cache_rate,\n",
    "            num_workers=num_workers,\n",
    "            progress=progress,\n",
    "        )\n",
    "\n",
    "    def randomize(self, data: np.ndarray) -> None:\n",
    "        self.R.shuffle(data)\n",
    "\n",
    "    def _generate_data_list(self):\n",
    "        datalist = []\n",
    "        with open(self.csv_file, mode='r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if self.section == 'real':\n",
    "                    image_path = os.path.join(self.root_dir, f\"real_{row['Filename']}\")\n",
    "                elif self.section == 'generated':\n",
    "                    image_path = os.path.join(self.root_dir, f\"generated_{row['Filename']}\")\n",
    "                if not os.path.exists(image_path):\n",
    "                    continue\n",
    "                image = np.array(Image.open(image_path).convert(\"L\"))\n",
    "                age = np.array([float(row['Age'])]).astype('float32')\n",
    "                datalist.append({\n",
    "                    \"image\": image,\n",
    "                    \"age\": age\n",
    "                })\n",
    "\n",
    "        return datalist\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a014dfa-3db9-4738-b713-58b7a59f348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim='no_channel'),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "        Lambdad(keys=[\"age\"], func=lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "        RepeatChanneld(keys=[\"image\"], repeats=3),  # (1, H, W) -> (3, H, W)\n",
    "        Transposed(keys=[\"image\"], indices=(1, 2, 0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0653e2a-646b-4032-ba89-dfce39ca9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./prediction_results\"\n",
    "csv_file = \"./validation.csv\"\n",
    "\n",
    "guidance_scales = [7.0, 5.0, 3.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28dbfef-e175-418f-b328-34b35e2cd671",
   "metadata": {},
   "source": [
    "### Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda7d99-319a-42db-9038-fd24fb58843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real\n",
    "real_data_dir = \"./real_images/275\"\n",
    "csv_file = \"./validation.csv\"\n",
    "\n",
    "real_ds = TestDataset(root_dir=real_data_dir, csv_file=csv_file, transform=transforms, section='real')\n",
    "real_loader = DataLoader(real_ds, batch_size=8, shuffle=False, num_workers=8, persistent_workers=True)\n",
    "\n",
    "real_images, real_ages = pytorch_to_numpy(real_loader)\n",
    "\n",
    "# 예측 수행 및 결과 저장\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "y_pred = model.predict(real_images)\n",
    "predictions = np.array(y_pred.flatten(), dtype=np.float32)  # 모델이 반환하는 형태에 따라 조정 가능\n",
    "ground_truths = np.array([gt.item() for gt in real_ages], dtype=np.float32)\n",
    "\n",
    "# 예측값과 실제값을 데이터프레임으로 저장\n",
    "prediction_data = pd.DataFrame({'Ground_Truth': ground_truths, 'Prediction': predictions})\n",
    "prediction_data.to_csv('./prediction_results/real.csv', index=False)\n",
    "\n",
    "# 정확도 평가 (MAE, MSE, RMSE)\n",
    "print(\"Real\")\n",
    "mae = np.mean(np.abs(predictions - ground_truths))\n",
    "mse = np.mean((predictions - ground_truths)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "r, _ = pearsonr(ground_truths, predictions)\n",
    "print(f'MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, Pearson r: {r:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac6d08-840e-4833-987f-105ddd3ef2f9",
   "metadata": {},
   "source": [
    "### Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964b54a-ccf5-47f3-8581-b3a3575e22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for guidance in guidance_scales:\n",
    "    print(f\"\\n### Processing Guidance Scale: {guidance} ###\")\n",
    "\n",
    "    generated_data_dir = f\"./generated_images/275_{guidance}\"\n",
    "\n",
    "    generated_ds = TestDataset(root_dir=generated_data_dir, csv_file=csv_file, transform=transforms, section='generated')\n",
    "    generated_loader = DataLoader(generated_ds, batch_size=8, shuffle=False, num_workers=8, persistent_workers=True)\n",
    "    \n",
    "    generated_images, generated_ages = pytorch_to_numpy(generated_loader)\n",
    "    \n",
    "    # 예측 수행 및 결과 저장\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    y_pred = model.predict(generated_images)\n",
    "    predictions = np.array(y_pred.flatten(), dtype=np.float32)  # 모델이 반환하는 형태에 따라 조정 가능\n",
    "    ground_truths = np.array([gt.item() for gt in generated_ages], dtype=np.float32)\n",
    "    \n",
    "    # 예측값과 실제값을 데이터프레임으로 저장\n",
    "    prediction_data = pd.DataFrame({'Ground_Truth': ground_truths, 'Prediction': predictions})\n",
    "    prediction_data.to_csv(f'./{output_dir}/{guidance}.csv', index=False)\n",
    "    \n",
    "    # 정확도 평가 (MAE, MSE, RMSE)\n",
    "    mae = np.mean(np.abs(predictions - ground_truths))\n",
    "    mse = np.mean((predictions - ground_truths)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r, _ = pearsonr(ground_truths, predictions)\n",
    "    print(f'MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, Pearson r: {r:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
